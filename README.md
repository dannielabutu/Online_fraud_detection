# Online_fraud_detection
This is my project work for artificial intelligence course unit which i used four machine learning model to evaluate the performance of the models a pick the best model for the project.
# ABSTRACT
The rise of online payment systems has led to an increase in payment fraud, particularly in credit card fraud. Credit card fraud involves theft or fraud using or during payment with a credit card. Online payment fraud is a significant concern for financial institutions as it affects customer trust and can cause monetary loss. In this project, we use a comprehensive evaluation method to compare the performance of four different machine learning models on the same online payment transaction information dataset. The performance of each model was evaluated using the Area under the ROC Curve (AUC), classification report, and confusion matrix. The four models are Random Forest, Decision Tree, Logistic Regression and AdaBoost. In this project, a dataset contains over six million online payment transactions were preprocessed and divided into 80% training and 20% testing. The data was heavily skewed so we used random undersampling to balance our dataset by randomly removing samples from the majority class and evaluated the performance of the four models. The performance of the models in term of AUC-ROC can be ranked as RF 99.76% > DT 99.73% >AB 97.57% > LR 76.21% and in terms of execution time, DT > LR>AB>RF. According to the comparison results, Random Forest has the highest AUC-ROC and consistency in predicting fraudulent transactions within this dataset. Selecting the most important feature for the model reduces the execution time to predict our result, increases efficiency and provides a more accurate result. After performing feature selection, we were able to achieve an AUC of 100% for Random Forest which indicates a perfect classifier. Therefore, it is recommended as the primary machine learning algorithm for online payment fraud detection.

# EVALUATION
There are 6,362,620 transactions, the data set has 11 attributes which include;
I. Type of transactions
II. Amount transacted
III. Customer ID and Recipient ID
IV. Old and New balance of Customer and Recipient
V. The time step of the transaction
VI. Whether the transaction was fraudulent or not
# FEATURE ENGINEERING AND DATA CLEANING
We performed exploratory data analysis (EDA) on a dataset to identify patterns in fraudulent transactions. We found that fraud only occurred in 'TRANSFER and 'CASH_OUT, so we assembled corresponding data for analysis. We dropped irrelevant columns and calculated errors in account balances before and after transactions, which made us to add a new feature called 'errorbalance' to identify fraudulent transactions. We converted the step feature from hours to days and created a new column called hour of the day for better understanding. We used one-hot encoding to convert categorical data in the type column to numerical data. The dataset was unbalanced, with fraudulent transactions making up only 0.13% of the total transactions. To address this problem, we used the standard scaler method from sci-kit-learn to scale the data.
# MODEL SELECTION AND PERFORMANCE ANALYSIS
We split our data into 80% training and 20% testing and evaluated the performance of our model on unbalanced data before using the random undersampling technique to balance the data. To accurately evaluate the performance of our model, we used three evaluation metrics namely, classification report, confusion matrix and AUC-ROC.
A classification report is a performance evaluation metric used to show the precision, recall, F1 Score, and support of a classification model.
Recall = ð‘‡ð‘ƒ ð‘‡ð‘ƒ+ð¹ð‘
Precision = ð‘‡ð‘ƒ ð‘‡ð‘ƒ+ð¹ð‘ƒ
F1 = 2 * ð‘ƒð‘Ÿð‘’ð‘ð‘ ð‘–ð‘œð‘› âˆ— ð‘…ð‘’ð‘ð‘Žð‘™ð‘™ ð‘ƒð‘Ÿð‘’ð‘ð‘ ð‘–ð‘œð‘›+ ð‘…ð‘’ð‘ð‘Žð‘™ð‘™
Accuracy = ð‘‡ð‘ƒ+ð‘‡ð‘ ð‘‡ð‘ƒ+ð‘‡ð‘+ð¹ð‘ƒ+ð¹ð‘

The ROC curve is a graphical representation of the performance of a binary classifier. It plots the true positive rate (sensitivity/recall) against the false positive rate (1 - specificity) at various threshold settings.
The AUC is a performance metric for machine learning models that measures the overall accuracy across all threshold settings. A score of 1 means perfect classification, while 0.5 means no better than random. Confusion matrix, classification report, and ROC curve analysis help evaluate model performance beyond accuracy, showing how well the model performs across various metrics.

# RESULT AND DISCUSSION
To evaluate the performance of the classification model, python programming on an Anaconda Jupiter notebook 3.9.12, on a MacBook retina, Dual-Core Intel Core M 8 GB 1600 MHz DDR31.1 GHZ have been used. Table 1 shows the confusion matrix of the models on skewed data. From Table 1 we can see that random forest has less error in labelling fraudulent transactions as valid compared to DT, RF and AB, but we cannot use this result because the data is heavily skewed. Table 2 shows the classification report for all the models on unbalanced data. The precision, recall and f1-score from Table 2 of random forest shows how well the model performs but since the data is unbalanced, we are not going to use this report as part of our evaluation. Table 3 shows the result of the confusion matrix for the balanced data. A random undersampling technique was used to balance the data by reducing the majority class. From Table 3, we can see that RF classified 2 fraudulent transactions as valid and 4 valid transactions as fraudulent compared to DT, LR and AB, hence we can rank the models as RF>DT>AB>LR. Table 4 shows the classification report for balanced data and RF performed better, followed by DT. Table 5 shows the result for AUC before and after the sampling technique. Although there's not much difference between RF and DT as RF has an AUC of 99.67% and DT has an AUC of 99.66%, this result cannot be used to determine the efficiency of our model as the data is heavily skewed. After performing random under-sampling, RF gave a higher AUC closer to 1 compared to DT, LR and AB with an AUC of 99.76%, hence the 4 models can be ranked as RF > DT>AB>LR. The execution time for the models is ranked, DT > LR>AB>RF. Table 6 shows the result for the confusion matrix after feature selection. Selecting the most important feature for the model reduces the execution time to predict our result, increases efficiency and provides a more accurate result. From Table 6, we can see that RF gave a significantly excellent result. The number of fraudulent transactions classified as valid was 0 and the number of valid transactions classified as fraudulent was 0. Table 7 shows the classification report for RF after feature selection and we can see how well the model performed after feature selection. Table 8 shows the AUC score after feature selection. From Table 8 below, we can see that RF gave an AUC score of 1, which indicates a perfect classifier.
# CONCLUSION AND FUTURE WORK
In this project, we used four supervised classification models to classify if a transaction is fraudulent or not, and RF gave a more accurate result compared to DT, LR and AB. This project shows the effectiveness of the model and applying 80:20 to test and train data yielded a good result. Although the execution time for RF is higher, it gave a more accurate result. There is not much significant difference between the performance of the models which answers our question for the hypothesis. Applying different performance metrics (confusion matrix, classification report and AUC_ROC) to evaluate the performance of the models provided a more comprehensive evaluation of our model. Feature selection yielded a more accurate result as it increased the performance, efficiency and execution time of the model to achieve an AUC of 1 which shows a perfect classifier. By combining four different models, we saw the strength and weaknesses of the models. It is recommended to test the models on multiple datasets with different characteristics to obtain a more general evaluation of their performance. Future work can be done by combing neural networks with binary classifiers and turning in hyperparameters to improve the execution time of the models. Additionally, future work can be done by using different sampling techniques on the models.
